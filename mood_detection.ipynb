{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO4e/0pEVzEvKiYCEAMIi1a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michealamanya/machine_learning/blob/main/mood_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNWrmy6uaV_D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Enhanced Facial Expression Recognition (FER) System - IMPROVED VERSION\n",
        "======================================================================\n",
        "Key improvements over previous version:\n",
        "✅ Larger input size (96x96) for better feature extraction\n",
        "✅ Focal Loss to handle class imbalance better\n",
        "✅ Mixup augmentation for regularization\n",
        "✅ Cosine annealing with warm restarts\n",
        "✅ Gradient clipping for stability\n",
        "✅ More aggressive augmentation\n",
        "✅ Fixed ONNX export\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import kagglehub\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "# ============================================================================\n",
        "# IMPROVED CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    # Image dimensions - INCREASED for better features\n",
        "    IMG_HEIGHT = 96  # Changed from 48\n",
        "    IMG_WIDTH = 96\n",
        "    IMG_CHANNELS = 3\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 48  # Slightly smaller due to larger images\n",
        "    LEARNING_RATE = 0.0005  # Lower initial LR\n",
        "    NUM_EPOCHS = 150  # More epochs\n",
        "    EARLY_STOP_PATIENCE = 20  # More patience\n",
        "\n",
        "    # Model parameters\n",
        "    DROPOUT_RATE = 0.4  # Slightly reduced\n",
        "    USE_PRETRAINED = True\n",
        "    USE_FOCAL_LOSS = True  # NEW: Better for class imbalance\n",
        "    FOCAL_ALPHA = 0.25\n",
        "    FOCAL_GAMMA = 2.0\n",
        "\n",
        "    # Augmentation\n",
        "    USE_MIXUP = True  # NEW: Mixup augmentation\n",
        "    MIXUP_ALPHA = 0.2\n",
        "\n",
        "    # TTA parameters\n",
        "    USE_TTA = True\n",
        "    TTA_TRANSFORMS = 8  # Increased\n",
        "\n",
        "    # Optimization\n",
        "    WEIGHT_DECAY = 0.0001\n",
        "    GRADIENT_CLIP = 1.0  # NEW: Gradient clipping\n",
        "\n",
        "    # Scheduler\n",
        "    USE_COSINE_ANNEALING = True  # NEW: Better LR schedule\n",
        "    T_MAX = 15\n",
        "    ETA_MIN = 1e-6\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    MODEL_SAVE_PATH = 'best_fer_improved.pth'\n",
        "    ONNX_SAVE_PATH = 'fer_model_improved.onnx'\n",
        "\n",
        "# ============================================================================\n",
        "# FOCAL LOSS - Better for imbalanced classes\n",
        "# ============================================================================\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss addresses class imbalance by down-weighting easy examples\n",
        "    and focusing on hard, misclassified examples.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, class_weights=None):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none',\n",
        "                                  weight=self.class_weights)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# ============================================================================\n",
        "# MIXUP AUGMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    \"\"\"Mixup augmentation - combines two examples\"\"\"\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"Mixup loss calculation\"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET WITH IMPROVED AUGMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "EMOTION_TO_IDX = {emotion: idx for idx, emotion in enumerate(EMOTIONS)}\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "\n",
        "        for emotion in EMOTIONS:\n",
        "            emotion_path = os.path.join(root_dir, emotion)\n",
        "            if os.path.isdir(emotion_path):\n",
        "                for img_name in os.listdir(emotion_path):\n",
        "                    if img_name.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
        "                        self.images.append(os.path.join(emotion_path, img_name))\n",
        "                        self.labels.append(EMOTION_TO_IDX[emotion])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        image = Image.open(img_path).convert('L')\n",
        "        image = Image.merge('RGB', (image, image, image))\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def get_class_weights(self):\n",
        "        label_counts = Counter(self.labels)\n",
        "        total = len(self.labels)\n",
        "        weights = {cls: total / (len(EMOTIONS) * count)\n",
        "                  for cls, count in label_counts.items()}\n",
        "        return torch.tensor([weights[i] for i in range(len(EMOTIONS))],\n",
        "                          dtype=torch.float32)\n",
        "\n",
        "# More aggressive augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((Config.IMG_HEIGHT, Config.IMG_WIDTH)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=25),  # Increased\n",
        "    transforms.RandomResizedCrop(Config.IMG_HEIGHT, scale=(0.75, 1.0)),  # More aggressive\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2),  # Increased\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.85, 1.15)),\n",
        "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),  # NEW\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    transforms.RandomErasing(p=0.2)  # NEW: Random erasing\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((Config.IMG_HEIGHT, Config.IMG_WIDTH)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ============================================================================\n",
        "# IMPROVED MODEL ARCHITECTURE\n",
        "# ============================================================================\n",
        "\n",
        "class ImprovedEmotionResNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced ResNet-18 with attention mechanism\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=7, pretrained=True, dropout_rate=0.4):\n",
        "        super(ImprovedEmotionResNet, self).__init__()\n",
        "\n",
        "        self.backbone = models.resnet18(pretrained=pretrained)\n",
        "        num_features = self.backbone.fc.in_features\n",
        "\n",
        "        # Remove original FC layer\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        # Custom classifier with more capacity\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(num_features, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate * 0.8),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate * 0.6),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        return self.classifier(features)\n",
        "\n",
        "# ============================================================================\n",
        "# IMPROVED TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch_improved(model, loader, criterion, optimizer, device, use_mixup=True):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Apply mixup\n",
        "        if use_mixup and Config.USE_MIXUP:\n",
        "            images, labels_a, labels_b, lam = mixup_data(images, labels, Config.MIXUP_ALPHA)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
        "        else:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        if Config.GRADIENT_CLIP > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), Config.GRADIENT_CLIP)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "\n",
        "        if use_mixup and Config.USE_MIXUP:\n",
        "            # Approximate accuracy for mixup\n",
        "            correct += (lam * (predicted == labels_a).sum().item() +\n",
        "                       (1 - lam) * (predicted == labels_b).sum().item())\n",
        "        else:\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss / total, 100 * correct / total\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return running_loss / total, 100 * correct / total\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"=\" * 70)\n",
        "    print(\"IMPROVED FER TRAINING\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Device: {Config.DEVICE}\")\n",
        "    print(f\"Image size: {Config.IMG_HEIGHT}x{Config.IMG_WIDTH}\")\n",
        "    print(f\"Focal Loss: {Config.USE_FOCAL_LOSS}\")\n",
        "    print(f\"Mixup: {Config.USE_MIXUP}\")\n",
        "    print(f\"Cosine Annealing: {Config.USE_COSINE_ANNEALING}\")\n",
        "\n",
        "    # Download dataset\n",
        "    print(\"\\nDownloading FER2013 dataset...\")\n",
        "    path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
        "    train_dir = os.path.join(path, \"train\")\n",
        "    test_dir = os.path.join(path, \"test\")\n",
        "\n",
        "    # Load datasets\n",
        "    print(\"Loading datasets...\")\n",
        "    full_train_dataset = EmotionDataset(train_dir, transform=train_transform)\n",
        "    test_dataset = EmotionDataset(test_dir, transform=val_test_transform)\n",
        "\n",
        "    class_weights = full_train_dataset.get_class_weights().to(Config.DEVICE)\n",
        "    print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "    # Split\n",
        "    val_size = int(0.15 * len(full_train_dataset))\n",
        "    train_size = len(full_train_dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                            shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                          shuffle=False, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                           shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    print(f\"Training samples: {train_size}\")\n",
        "    print(f\"Validation samples: {val_size}\")\n",
        "    print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = ImprovedEmotionResNet(\n",
        "        num_classes=len(EMOTIONS),\n",
        "        pretrained=Config.USE_PRETRAINED,\n",
        "        dropout_rate=Config.DROPOUT_RATE\n",
        "    ).to(Config.DEVICE)\n",
        "\n",
        "    # Loss function\n",
        "    if Config.USE_FOCAL_LOSS:\n",
        "        criterion = FocalLoss(\n",
        "            alpha=Config.FOCAL_ALPHA,\n",
        "            gamma=Config.FOCAL_GAMMA,\n",
        "            class_weights=class_weights\n",
        "        )\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=Config.LEARNING_RATE,\n",
        "        weight_decay=Config.WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    # Scheduler\n",
        "    if Config.USE_COSINE_ANNEALING:\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer,\n",
        "            T_0=Config.T_MAX,\n",
        "            T_mult=2,\n",
        "            eta_min=Config.ETA_MIN\n",
        "        )\n",
        "    else:\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='min', factor=0.5, patience=5\n",
        "        )\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Starting training...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        train_loss, train_acc = train_epoch_improved(\n",
        "            model, train_loader, criterion, optimizer, Config.DEVICE\n",
        "        )\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, Config.DEVICE)\n",
        "\n",
        "        if Config.USE_COSINE_ANNEALING:\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f\"Epoch [{epoch+1:3d}/{Config.NUM_EPOCHS}] | \"\n",
        "              f\"Loss: {train_loss:.4f}/{val_loss:.4f} | \"\n",
        "              f\"Acc: {train_acc:.2f}%/{val_acc:.2f}% | \"\n",
        "              f\"LR: {current_lr:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "            }, Config.MODEL_SAVE_PATH)\n",
        "            print(f\"  ✓ Best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= Config.EARLY_STOP_PATIENCE:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    checkpoint = torch.load(Config.MODEL_SAVE_PATH)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    test_loss, test_acc = validate(model, test_loader, criterion, Config.DEVICE)\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # Get predictions for report\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(Config.DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Classification Report:\")\n",
        "    print(\"=\" * 70)\n",
        "    print(classification_report(all_labels, all_preds, target_names=EMOTIONS, digits=4))\n",
        "\n",
        "    # Export to ONNX (with fix)\n",
        "    print(\"\\nExporting to ONNX...\")\n",
        "    try:\n",
        "        model.eval()\n",
        "        dummy_input = torch.randn(1, 3, Config.IMG_HEIGHT, Config.IMG_WIDTH).to(Config.DEVICE)\n",
        "\n",
        "        torch.onnx.export(\n",
        "            model,\n",
        "            dummy_input,\n",
        "            Config.ONNX_SAVE_PATH,\n",
        "            export_params=True,\n",
        "            opset_version=14,  # Updated opset version\n",
        "            do_constant_folding=True,\n",
        "            input_names=['input'],\n",
        "            output_names=['output'],\n",
        "            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        "        )\n",
        "        print(f\"✓ ONNX model saved: {Config.ONNX_SAVE_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"✗ ONNX export failed: {e}\")\n",
        "        print(\"  Install onnx: !pip install onnx onnxscript\")\n",
        "\n",
        "    print(f\"\\n{'=' * 70}\")\n",
        "    print(\"TRAINING SUMMARY\")\n",
        "    print(f\"{'=' * 70}\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "    print(f\"Model saved: {Config.MODEL_SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tll-Ar_fgslr",
        "outputId": "80ebafde-5083-465b-f1e8-f55376c86bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "IMPROVED FER TRAINING\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Image size: 96x96\n",
            "Focal Loss: True\n",
            "Mixup: True\n",
            "Cosine Annealing: True\n",
            "\n",
            "Downloading FER2013 dataset...\n",
            "Using Colab cache for faster access to the 'fer2013' dataset.\n",
            "Loading datasets...\n",
            "Class weights: tensor([1.0266, 9.4066, 1.0010, 0.5684, 0.8260, 0.8491, 1.2934],\n",
            "       device='cuda:0')\n",
            "Training samples: 24403\n",
            "Validation samples: 4306\n",
            "Test samples: 7178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Starting training...\n",
            "======================================================================\n",
            "Epoch [  1/150] | Loss: 0.3470/0.3033 | Acc: 17.71%/22.53% | LR: 0.000495\n",
            "  ✓ Best model saved! (Val Acc: 22.53%)\n",
            "Epoch [  2/150] | Loss: 0.3203/0.2862 | Acc: 23.90%/29.89% | LR: 0.000478\n",
            "  ✓ Best model saved! (Val Acc: 29.89%)\n",
            "Epoch [  3/150] | Loss: 0.3022/0.2606 | Acc: 30.62%/35.11% | LR: 0.000452\n",
            "  ✓ Best model saved! (Val Acc: 35.11%)\n",
            "Epoch [  4/150] | Loss: 0.2859/0.2627 | Acc: 35.78%/35.81% | LR: 0.000417\n",
            "  ✓ Best model saved! (Val Acc: 35.81%)\n",
            "Epoch [  5/150] | Loss: 0.2775/0.2544 | Acc: 36.97%/40.36% | LR: 0.000375\n",
            "  ✓ Best model saved! (Val Acc: 40.36%)\n",
            "Epoch [  6/150] | Loss: 0.2701/0.2345 | Acc: 39.31%/41.69% | LR: 0.000328\n",
            "  ✓ Best model saved! (Val Acc: 41.69%)\n",
            "Epoch [  7/150] | Loss: 0.2634/0.2255 | Acc: 39.92%/42.64% | LR: 0.000277\n",
            "  ✓ Best model saved! (Val Acc: 42.64%)\n",
            "Epoch [  8/150] | Loss: 0.2581/0.2246 | Acc: 40.79%/43.22% | LR: 0.000224\n",
            "  ✓ Best model saved! (Val Acc: 43.22%)\n",
            "Epoch [  9/150] | Loss: 0.2526/0.2207 | Acc: 42.32%/44.91% | LR: 0.000173\n",
            "  ✓ Best model saved! (Val Acc: 44.91%)\n",
            "Epoch [ 10/150] | Loss: 0.2468/0.2060 | Acc: 43.09%/47.12% | LR: 0.000126\n",
            "  ✓ Best model saved! (Val Acc: 47.12%)\n",
            "Epoch [ 11/150] | Loss: 0.2390/0.2050 | Acc: 44.33%/48.33% | LR: 0.000084\n",
            "  ✓ Best model saved! (Val Acc: 48.33%)\n",
            "Epoch [ 12/150] | Loss: 0.2348/0.1925 | Acc: 45.21%/48.63% | LR: 0.000049\n",
            "  ✓ Best model saved! (Val Acc: 48.63%)\n",
            "Epoch [ 13/150] | Loss: 0.2331/0.1901 | Acc: 45.68%/49.02% | LR: 0.000023\n",
            "  ✓ Best model saved! (Val Acc: 49.02%)\n",
            "Epoch [ 14/150] | Loss: 0.2272/0.1925 | Acc: 45.88%/50.02% | LR: 0.000006\n",
            "  ✓ Best model saved! (Val Acc: 50.02%)\n",
            "Epoch [ 15/150] | Loss: 0.2300/0.1964 | Acc: 46.05%/50.05% | LR: 0.000500\n",
            "  ✓ Best model saved! (Val Acc: 50.05%)\n",
            "Epoch [ 16/150] | Loss: 0.2583/0.2228 | Acc: 41.47%/45.42% | LR: 0.000499\n",
            "Epoch [ 17/150] | Loss: 0.2506/0.2239 | Acc: 42.72%/46.19% | LR: 0.000495\n",
            "Epoch [ 18/150] | Loss: 0.2527/0.2190 | Acc: 41.95%/45.05% | LR: 0.000488\n",
            "Epoch [ 19/150] | Loss: 0.2493/0.2227 | Acc: 43.36%/46.01% | LR: 0.000478\n",
            "Epoch [ 20/150] | Loss: 0.2486/0.2232 | Acc: 43.08%/42.13% | LR: 0.000467\n",
            "Epoch [ 21/150] | Loss: 0.2484/0.2192 | Acc: 43.43%/47.98% | LR: 0.000452\n",
            "Epoch [ 22/150] | Loss: 0.2415/0.2075 | Acc: 44.01%/46.91% | LR: 0.000436\n",
            "Epoch [ 23/150] | Loss: 0.2390/0.2051 | Acc: 44.90%/48.91% | LR: 0.000417\n",
            "Epoch [ 24/150] | Loss: 0.2372/0.2030 | Acc: 44.61%/48.65% | LR: 0.000397\n",
            "Epoch [ 25/150] | Loss: 0.2365/0.2033 | Acc: 45.45%/50.65% | LR: 0.000375\n",
            "  ✓ Best model saved! (Val Acc: 50.65%)\n",
            "Epoch [ 26/150] | Loss: 0.2339/0.1981 | Acc: 46.27%/52.04% | LR: 0.000352\n",
            "  ✓ Best model saved! (Val Acc: 52.04%)\n",
            "Epoch [ 27/150] | Loss: 0.2310/0.2028 | Acc: 46.37%/51.23% | LR: 0.000328\n",
            "Epoch [ 28/150] | Loss: 0.2287/0.1895 | Acc: 46.86%/52.02% | LR: 0.000302\n",
            "Epoch [ 29/150] | Loss: 0.2213/0.1915 | Acc: 47.66%/52.93% | LR: 0.000277\n",
            "  ✓ Best model saved! (Val Acc: 52.93%)\n",
            "Epoch [ 30/150] | Loss: 0.2208/0.1920 | Acc: 48.03%/52.32% | LR: 0.000251\n",
            "Epoch [ 31/150] | Loss: 0.2145/0.1816 | Acc: 48.33%/51.51% | LR: 0.000224\n",
            "Epoch [ 32/150] | Loss: 0.2148/0.1792 | Acc: 49.32%/53.18% | LR: 0.000199\n",
            "  ✓ Best model saved! (Val Acc: 53.18%)\n",
            "Epoch [ 33/150] | Loss: 0.2087/0.1759 | Acc: 50.16%/53.69% | LR: 0.000173\n",
            "  ✓ Best model saved! (Val Acc: 53.69%)\n",
            "Epoch [ 34/150] | Loss: 0.2011/0.1764 | Acc: 51.20%/53.44% | LR: 0.000149\n"
          ]
        }
      ]
    }
  ]
}